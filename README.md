# Web-app-LLM
Run a LLM model locally with Streamlit and Ollama.
